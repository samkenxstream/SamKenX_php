{"id":"speech\/v1p1beta1\/recognitionconfig\/audioencoding","type":"","title":"Google\\Cloud\\Speech\\V1p1beta1\\RecognitionConfig\\AudioEncoding","name":"AudioEncoding","description":"<p>The encoding of the audio data sent in the request.<\/p>\n<p>All encodings support only 1 channel (mono) audio, unless the\n<code>audio_channel_count<\/code> and <code>enable_separate_recognition_per_channel<\/code> fields\nare set.\nFor best results, the audio source should be captured and transmitted using\na lossless encoding (<code>FLAC<\/code> or <code>LINEAR16<\/code>). The accuracy of the speech\nrecognition can be reduced if lossy codecs are used to capture or transmit\naudio, particularly if background noise is present. Lossy codecs include\n<code>MULAW<\/code>, <code>AMR<\/code>, <code>AMR_WB<\/code>, <code>OGG_OPUS<\/code>, <code>SPEEX_WITH_HEADER_BYTE<\/code>, <code>MP3<\/code>,\nand <code>WEBM_OPUS<\/code>.\nThe <code>FLAC<\/code> and <code>WAV<\/code> audio file formats include a header that describes the\nincluded audio content. You can request recognition for <code>WAV<\/code> files that\ncontain either <code>LINEAR16<\/code> or <code>MULAW<\/code> encoded audio.\nIf you send <code>FLAC<\/code> or <code>WAV<\/code> audio file format in\nyour request, you do not need to specify an <code>AudioEncoding<\/code>; the audio\nencoding format is determined from the file header. If you specify\nan <code>AudioEncoding<\/code> when you send  send <code>FLAC<\/code> or <code>WAV<\/code> audio, the\nencoding configuration must match the encoding described in the audio\nheader; otherwise the request returns an\n[google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error code.<\/p>\n<p>Protobuf type <code>google.cloud.speech.v1p1beta1.RecognitionConfig.AudioEncoding<\/code><\/p>","examples":[],"resources":[],"methods":[]}